{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import ir_datasets\n",
    "import time\n",
    "import heapq\n",
    "from rerankers import Reranker\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Split em conjuntos**\n",
    "\n",
    "Datasets de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "def load_dataset(input_file):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "data_set = \"../subset_msmarco_train_0/subset_msmarco_train_0.01_99.pkl\"\n",
    "\n",
    "data = load_dataset(data_set)\n",
    "queries = data[\"queries\"]\n",
    "documents = data[\"docs\"]\n",
    "\n",
    "# Split the queries (queries is a dictionary of {query_id: query_object})\n",
    "query_ids = list(queries.keys())  # List of query IDs\n",
    "\n",
    "# Shuffle query IDs to ensure a random split\n",
    "random.shuffle(query_ids)\n",
    "\n",
    "# Split into 80% for training, 20% for validation\n",
    "split_ratio = 0.8\n",
    "train_query_ids = query_ids[:int(len(query_ids) * split_ratio)]\n",
    "test_query_ids = query_ids[int(len(query_ids) * split_ratio):]\n",
    "\n",
    "train_queries = {qid: queries[qid] for qid in train_query_ids}\n",
    "test_queries = {qid: queries[qid] for qid in test_query_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tratamento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário agrupa os documentos relevantes por consulta (query)\n",
    "relevant_docs = dict()\n",
    "\n",
    "for qrel in data[\"qrels\"]:\n",
    "    relevant_docs[qrel.query_id] = relevant_docs.get(qrel.query_id, []) + [qrel.doc_id]\n",
    "\n",
    "# e.g.: relevant_docs = {'query_id': ['doc_id_1', 'doc_id_12']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica os documentos relevantes no conjunto de treinamento \n",
    "# removendo duplicatas\n",
    "train_docs = set()\n",
    "for qid in train_query_ids:\n",
    "    train_docs.update(relevant_docs[qid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **mean reciprocal rank** is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: $1$ for first place, $\\frac{1}{2}$ for second place, $\\frac{1}{3}$ for third place and so on. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:\n",
    "\n",
    "$$\n",
    "MRR =\n",
    "\\frac{1}{\\|Q\\|} = \\sum_{i=1}^{\\|Q\\|} \\frac{1}{rank_i}\n",
    "$$\n",
    "\n",
    "where ${\\displaystyle {\\text{rank}}_{i}}$ refers to the rank position of the first relevant document for the i-th query.\n",
    "\n",
    "The reciprocal value of the mean reciprocal rank corresponds to the harmonic mean of the ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMRR(query_id, ranking, relevant_docs, top_k=None):\n",
    "    \"\"\"\n",
    "    Calcula o MRR (Mean Reciprocal Rank) para uma consulta.\n",
    "    \n",
    "    :param query_id: ID da consulta atual.\n",
    "    :param ranking: Lista de documentos recuperados ordenados (tuplas com o ID e a pontuação).\n",
    "    :param relevant_docs: Dicionário com os documentos relevantes para cada consulta.\n",
    "    :param top_k: Número de documentos a considerar. Se None, considera todos os documentos.\n",
    "    \n",
    "    :return: MRR para a consulta.\n",
    "    \"\"\"\n",
    "    \n",
    "    if top_k is not None:\n",
    "        ranking = ranking[:top_k]\n",
    "    \n",
    "    for i, doc in enumerate(ranking):\n",
    "        doc_id = doc[0]  # O primeiro elemento de cada tupla é o ID do documento\n",
    "        if doc_id in relevant_docs[query_id]:\n",
    "            return 1 / (i + 1)  # Retorna o recíproco da posição do primeiro documento relevante\n",
    "    \n",
    "    return 0  # Se não houver documento relevante nos primeiros K\n",
    "\n",
    "def calculate_recall_at_k(relevant_docs, doc_ranking, top_k=100):\n",
    "    \"\"\"\n",
    "    Calcular o Recall@K (com K=100) para uma consulta.\n",
    "    \"\"\"\n",
    "    \n",
    "    retrieved_relevant_docs = sum(1 for doc_id in doc_ranking[:top_k] if doc_id in relevant_docs)\n",
    "    \n",
    "    if relevant_docs:\n",
    "        return retrieved_relevant_docs / len(relevant_docs)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantDocTexts(query_id):\n",
    "    \"\"\"\n",
    "    Retorna os textos dos documentos relevantes para uma consulta.\n",
    "    \n",
    "    :param query_id: ID da query.\n",
    "    \n",
    "    :return: Lista de textos dos documentos relevantes.\n",
    "    \"\"\"\n",
    "    \n",
    "    relevant_doc_texts = []\n",
    "    for doc_id in relevant_docs[query_id]:\n",
    "        relevant_doc_texts.append(data[\"docs\"][doc_id].text)\n",
    "\n",
    "    return relevant_doc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, remove_punctuation = True, lowercase = True):\n",
    "    \"\"\"\n",
    "    Função para preprocessamento de texto: remove pontuação, converte para minúsculas e divide em tokens.\n",
    "    \n",
    "    :param text: Texto a ser processado.\n",
    "    :remove_punctuation bool: Para True remove pontuação\n",
    "    :lowercase bool: Para True coloca em lowercase\n",
    "\n",
    "    :return: Lista de tokens do texto.\n",
    "    \"\"\"\n",
    "\n",
    "    # [^\\w\\s] corresponde a qualquer caractere que não seja uma letra, número, underscore ou espaço em branco\n",
    "    # mantendo apenas letras, números e espaços\n",
    "\n",
    "    # Caso 1\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    # Caso 2\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelo baseline**\n",
    "\n",
    "## **BM25**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O BM25Okapi é uma implementação do BM25, um modelo clássico utilizado em sistemas de recuperação de informações (como motores de busca) para avaliar a relevância de documentos em relação a uma consulta (query).\n",
    "\n",
    "O BM25 é uma fórmula de pontuação de relevância que calcula o quão bem um documento corresponde a uma consulta com base nas palavras que o documento contém.\n",
    "\n",
    "Ele é um modelo probabilístico de recuperação de informações, baseado na ideia de que a relevância de um documento para uma consulta depende da frequência das palavras que aparecem no documento e na consulta, mas com uma diminuição das contribuições das palavras que ocorrem com frequência excessiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(query_id, bm25, remove_punctuation, lowercase, top_k=100):\n",
    "    \"\"\"\n",
    "        Usa o BM25 para recuperar o ranking de documentos relevantes para uma consulta.\n",
    "    \"\"\"\n",
    "\n",
    "    query = queries[query_id].text\n",
    "    tokenized_query = preprocess(query, remove_punctuation, lowercase)\n",
    "\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Manter os top_k documentos mais relevantes usando um heap\n",
    "    doc_ranking = heapq.nlargest(top_k, zip(data[\"docs\"].keys(), doc_scores), key=lambda x: x[1])\n",
    "\n",
    "    return doc_ranking\n",
    "\n",
    "\n",
    "def evaluate_model(query_ids, relevant_docs, remove_punctuation, lowercase, documents, top_k=100):\n",
    "    \"\"\"\n",
    "    Avalia o modelo em termos de MRR@K, Recall@100, tempo de execução e tempo máximo de execução.\n",
    "\n",
    "    :param query_ids: IDs das consultas a serem avaliadas.\n",
    "    :param relevant_docs: Dicionário com documentos relevantes para cada consulta.\n",
    "    :complete\n",
    "    :param documents: Dicionário com documentos do modelo.\n",
    "    :param top_k: Número de documentos a considerar para Recall@100 e MRR@10.\n",
    "    \n",
    "    :return: Tuple com as métricas calculadas: (average_mrr, average_time, max_execution_time, average_recall_100)\n",
    "    \"\"\"\n",
    "    \n",
    "    total_mrr = 0\n",
    "    total_recall_100 = 0\n",
    "    total_execution = 0\n",
    "    max_execution = 0\n",
    "\n",
    "    tokenized_corpus = [preprocess(doc.text, remove_punctuation, lowercase) for doc in data[\"docs\"].values()]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    # Avaliar cada consulta\n",
    "    for query_id in query_ids:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Recupera o ranking de documentos para a consulta\n",
    "        doc_ranking = baseline_model(query_id, bm25, remove_punctuation, lowercase)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        total_execution += execution_time\n",
    "\n",
    "        # Atualiza o tempo máximo de execução\n",
    "        if execution_time > max_execution:\n",
    "            max_execution = execution_time\n",
    "\n",
    "        # Calcular MRR@10\n",
    "        mrr_for_query = calculateMRR(query_id, doc_ranking, relevant_docs, top_k=10)\n",
    "        total_mrr += mrr_for_query\n",
    "\n",
    "        # Calcular Recall@100\n",
    "        recall_for_query = calculate_recall_at_k(relevant_docs[query_id], doc_ranking, top_k=100)\n",
    "        total_recall_100 += recall_for_query\n",
    "\n",
    "    # Calcular as médias\n",
    "    average_mrr = total_mrr / len(query_ids)\n",
    "    average_time = total_execution / len(query_ids)\n",
    "    average_recall_100 = total_recall_100 / len(query_ids)\n",
    "\n",
    "    return average_mrr, average_time, max_execution, average_recall_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o conjunto de teste:\n",
      "MRR@10 médio: 0.480087945087945\n",
      "Tempo médio de execução por consulta: 0.41367159963728073\n",
      "Tempo máximo de execução por consulta: 1.2225031852722168\n",
      "Recall@100 médio: 0.0\n"
     ]
    }
   ],
   "source": [
    "average_mrr, average_time, max_execution, average_recall_100 = evaluate_model(\n",
    "    test_query_ids, \n",
    "    relevant_docs, \n",
    "    remove_punctuation=True, \n",
    "    lowercase=True, \n",
    "    documents=data[\"docs\"], \n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "results_data.append([\"Test\", len(test_query_ids), average_mrr, average_recall_100, average_time, max_execution])\n",
    "\n",
    "print(\"Resultados para o conjunto de teste:\")\n",
    "print(f\"MRR@10 médio: {average_mrr}\")\n",
    "print(f\"Tempo médio de execução por consulta: {average_time}\")\n",
    "print(f\"Tempo máximo de execução por consulta: {max_execution}\")\n",
    "print(f\"Recall@100 médio: {average_recall_100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o conjunto de treinamento:\n",
      "MRR@10 médio: 0.4421159604034162\n",
      "Tempo médio de execução por consulta: 0.42464258739664235\n",
      "Tempo máximo de execução por consulta: 1.8286347389221191\n",
      "Recall@100 médio: 0.0\n"
     ]
    }
   ],
   "source": [
    "average_mrr, average_time, max_execution, average_recall_100 = evaluate_model(\n",
    "    train_query_ids, \n",
    "    relevant_docs, \n",
    "    remove_punctuation=True, \n",
    "    lowercase=True, \n",
    "    documents=data[\"docs\"], \n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "results_data.append([\"Test\", len(test_query_ids), average_mrr, average_recall_100, average_time, max_execution])\n",
    "\n",
    "print(\"Resultados para o conjunto de treinamento:\")\n",
    "print(f\"MRR@10 médio: {average_mrr}\")\n",
    "print(f\"Tempo médio de execução por consulta: {average_time}\")\n",
    "print(f\"Tempo máximo de execução por consulta: {max_execution}\")\n",
    "print(f\"Recall@100 médio: {average_recall_100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mrr, average_time, max_execution, average_recall_100 = evaluate_model(\n",
    "    query_ids, \n",
    "    relevant_docs, \n",
    "    remove_punctuation=True, \n",
    "    lowercase=True, \n",
    "    documents=data[\"docs\"], \n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "results_data.append([\"Full\", len(test_query_ids), average_mrr, average_recall_100, average_time, max_execution])\n",
    "\n",
    "print(\"Resultados para o conjunto completo:\")\n",
    "print(f\"MRR@10 médio: {average_mrr}\")\n",
    "print(f\"Tempo médio de execução por consulta: {average_time}\")\n",
    "print(f\"Tempo máximo de execução por consulta: {max_execution}\")\n",
    "print(f\"Recall@100 médio: {average_recall_100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_25_results = pd.DataFrame(results_data, columns=[\"Dataset\", \"Size\", \"MRR@10\", \"Recall@100\", \"Average query runtime (sec)\", \"Maximum query runtime (sec)\"])\n",
    "bm_25_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado recente:\n",
    "- Com processamento dos documentos\n",
    "- Com processamento das queries\n",
    "\n",
    "Obs.: O processamento inclui remover qualquer caractere que não seja uma letra, número, underscore ou espaço em branco; e deixar em lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Estudo de escalabilidade:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Size</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Runtime (sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>555</td>\n",
       "      <td>0.488720</td>\n",
       "      <td>298.350545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train</td>\n",
       "      <td>2216</td>\n",
       "      <td>0.449240</td>\n",
       "      <td>1220.195126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Full</td>\n",
       "      <td>2771</td>\n",
       "      <td>0.457147</td>\n",
       "      <td>1467.348227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset  Size       MRR  Runtime (sec)\n",
       "0    Test   555  0.488720     298.350545\n",
       "1   Train  2216  0.449240    1220.195126\n",
       "2    Full  2771  0.457147    1467.348227"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"==> Estudo de escalabilidade:\")\n",
    "bm_25_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_25_results.to_csv(\"../results/bm_25_results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado anterior:\n",
    "- Com processamento dos documentos\n",
    "- Sem processamento das queries (apenas lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Size</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>555</td>\n",
       "      <td>0.474292</td>\n",
       "      <td>257.141707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train</td>\n",
       "      <td>2216</td>\n",
       "      <td>0.428867</td>\n",
       "      <td>1072.796115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Full</td>\n",
       "      <td>2771</td>\n",
       "      <td>0.437965</td>\n",
       "      <td>1340.438707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset  Size       MRR      Runtime\n",
       "0    Test   555  0.474292   257.141707\n",
       "1   Train  2216  0.428867  1072.796115\n",
       "2    Full  2771  0.437965  1340.438707"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"==> Estudo de escalabilidade:\")\n",
    "bm_25_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ablação na etapa de tratamento dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame para armazenar os resultados\n",
    "bm_25_processing_results = pd.DataFrame(columns=[\n",
    "    \"Remove punctuation\", \n",
    "    \"Lowercase\", \n",
    "    \"MRR@10\", \n",
    "    \"Recall@100\", \n",
    "    \"Average query runtime (sec)\", \n",
    "    \"Maximum query runtime (sec)\"\n",
    "])\n",
    "\n",
    "index = 0\n",
    "\n",
    "# Teste de ablação para as combinações de remove_punctuation e lowercase\n",
    "for remove_punctuation in [True, False]:\n",
    "    for lowercase in [True, False]:\n",
    "        # Variáveis para armazenar os resultados\n",
    "        average_mrr = 0\n",
    "        total_recall_100 = 0\n",
    "        total_execution = 0\n",
    "        max_execution = 0\n",
    "\n",
    "        # Tempo total de execução para o conjunto de teste\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Avaliar o modelo para cada consulta no conjunto de teste\n",
    "        for query_id in test_query_ids:\n",
    "            # Chama a função de avaliação com os parâmetros de pre-processamento\n",
    "            mrr, avg_time, max_time, recall_100 = evaluate_model(\n",
    "                [query_id],  # Passa a consulta individual\n",
    "                relevant_docs, \n",
    "                remove_punctuation=remove_punctuation, \n",
    "                lowercase=lowercase, \n",
    "                documents=data[\"docs\"], \n",
    "                top_k=10  # Para MRR@10\n",
    "            )\n",
    "            \n",
    "            # Acumula as métricas\n",
    "            average_mrr += mrr\n",
    "            total_recall_100 += recall_100\n",
    "            total_execution += avg_time\n",
    "            if max_time > max_execution:\n",
    "                max_execution = max_time\n",
    "\n",
    "        # Calcular médias\n",
    "        average_mrr /= len(test_query_ids)\n",
    "        average_recall_100 = total_recall_100 / len(test_query_ids)\n",
    "        average_time = total_execution / len(test_query_ids)\n",
    "\n",
    "        # Calcula o tempo total de execução\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "        # Armazenar os resultados no DataFrame\n",
    "        bm_25_processing_results.loc[index] = [\n",
    "            remove_punctuation, \n",
    "            lowercase, \n",
    "            average_mrr, \n",
    "            average_recall_100, \n",
    "            average_time, \n",
    "            max_execution\n",
    "        ]\n",
    "\n",
    "        index += 1\n",
    "\n",
    "print(bm_25_processing_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Estudo do desempenho do tratamento de dados:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Remove punctuation</th>\n",
       "      <th>Lowercase</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Runtime (sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.488720</td>\n",
       "      <td>297.235475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.488860</td>\n",
       "      <td>289.510962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.474292</td>\n",
       "      <td>297.298308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.474432</td>\n",
       "      <td>288.388891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Remove punctuation  Lowercase       MRR  Runtime (sec)\n",
       "0                True       True  0.488720     297.235475\n",
       "1                True      False  0.488860     289.510962\n",
       "2               False       True  0.474292     297.298308\n",
       "3               False      False  0.474432     288.388891"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"==> Estudo do desempenho do tratamento de dados:\")\n",
    "bm_25_processing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_25_processing_results.to_csv(\"../results/bm_25_processing_results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TF-IDF (Term Frequency-Inverse Document Frequency)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tfidf(documents):\n",
    "    \"\"\"\n",
    "        Inicializa o TF-IDF e vetoriza todos os documentos uma vez\n",
    "    \"\"\"\n",
    "\n",
    "    # Extrair o texto dos documentos\n",
    "    doc_texts = [doc.text for doc in documents.values()]\n",
    "    \n",
    "    # Inicializar o TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Vetorizar todos os documentos\n",
    "    tfidf_matrix = vectorizer.fit_transform(doc_texts)\n",
    "    \n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "\n",
    "def retrieve_relevant_documents(query, vectorizer, tfidf_matrix, documents, top_k=10):\n",
    "    \"\"\"\n",
    "        Recupera o documento mais relevante para uma consulta\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vetorizar a consulta\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "\n",
    "    # Calcular a similaridade de cosseno entre a consulta e os documentos\n",
    "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "\n",
    "    if top_k == 1:\n",
    "        # Encontrar o índice do documento mais relevante\n",
    "        most_relevant_document_index = cosine_similarities.argmax()\n",
    "\n",
    "        # Recuperar o ID do documento mais relevante\n",
    "        doc_ranking = [list(documents.keys())[most_relevant_document_index]]\n",
    "        \n",
    "    else:\n",
    "        # Usar heap para obter os top_k documentos mais relevantes\n",
    "        doc_ranking = heapq.nlargest(top_k, zip(documents.keys(), cosine_similarities[0]), key=lambda x: x[1])\n",
    "        doc_ranking = [doc_id for doc_id, score in doc_ranking]\n",
    "\n",
    "    return doc_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_model(query_ids, documents, relevant_docs):\n",
    "\n",
    "    vectorizer, tfidf_matrix = initialize_tfidf(documents)\n",
    "\n",
    "    total_mrr = 0\n",
    "    total_recall_100 = 0\n",
    "    max_execution = 0\n",
    "    total_execution = 0\n",
    "    top_k = 100  # Considera os 100 primeiros documentos\n",
    "\n",
    "    for query_id in query_ids:\n",
    "        start_time = time.time()\n",
    "\n",
    "        query = queries[query_id].text\n",
    "\n",
    "        # Recuperar os 10 documentos mais relevantes\n",
    "        doc_ranking = retrieve_relevant_documents(query, vectorizer, tfidf_matrix, documents, top_k)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        total_execution += execution_time\n",
    "\n",
    "        # Calcular o tempo máximo de execução\n",
    "        if execution_time > max_execution:\n",
    "            max_execution = execution_time\n",
    "\n",
    "        # Calcular MRR@10 (ou outro valor de K)\n",
    "        mrr_for_query = calculateMRR(query_id, doc_ranking, relevant_docs, top_k)\n",
    "        total_mrr += mrr_for_query\n",
    "\n",
    "        # Calcular Recall@100\n",
    "        recall_for_query = calculate_recall_at_k(relevant_docs[query_id], doc_ranking, top_k)\n",
    "        total_recall_100 += recall_for_query\n",
    "\n",
    "    # Calcular médias\n",
    "    average_mrr = total_mrr / len(query_ids)\n",
    "    average_time = total_execution / len(query_ids)\n",
    "    average_recall_100 = total_recall_100 / len(query_ids)\n",
    "\n",
    "    return average_mrr, average_time, max_execution, average_recall_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mrr, average_time, max_execution, average_recall_100 = tf_idf_model(query_ids, documents, relevant_docs)\n",
    "\n",
    "results_data = {\n",
    "    \"Average MRR@10\": [average_mrr],\n",
    "    \"Average Query Runtime (sec)\": [average_time],\n",
    "    \"Maximum Query Runtime (sec)\": [max_execution],\n",
    "    \"Average Recall@100\": [average_recall_100]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(f\"Average MRR@10: {average_mrr}\")\n",
    "print(f\"Average Query Runtime (sec): {average_time}\")\n",
    "print(f\"Maximum Query Runtime (sec): {max_execution}\")\n",
    "print(f\"Average Recall@100: {average_recall_100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"../results/tf_idf_processing_results.txt\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelo Reranker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default cross-encoder model for language en\n",
      "Warning: Model type could not be auto-mapped with the defaults list. Defaulting to TransformerRanker.\n",
      "If your model is NOT intended to be ran as a one-label cross-encoder, please reload it and specify the model_type! Otherwise, you may ignore this warning. You may specify `model_type='cross-encoder'` to suppress this warning in the future.\n",
      "Default Model: mixedbread-ai/mxbai-rerank-base-v1\n",
      "Loading TransformerRanker model mixedbread-ai/mxbai-rerank-base-v1 (this message can be suppressed by setting verbose=0)\n",
      "No dtype set\n",
      "Using dtype torch.float32\n",
      "Loaded model mixedbread-ai/mxbai-rerank-base-v1\n",
      "Using device cuda.\n",
      "Using dtype torch.float32.\n"
     ]
    }
   ],
   "source": [
    "ranker = Reranker(\"cross-encoder\", device='cuda')\n",
    "tokenized_corpus = [preprocess(doc.text, remove_punctuation, lowercase) for doc in data[\"docs\"].values()]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def model(query_id, remove_punctuation, lowercase):\n",
    "    query = queries[query_id].text\n",
    "    tokenized_query = preprocess(query, remove_punctuation, lowercase)\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    doc_ranking = sorted(zip(data[\"docs\"].keys(), doc_scores), key=lambda x: x[1], reverse=True)\n",
    "    top_10 = doc_ranking[:10]\n",
    "    top_10_ids = [doc_id for doc_id, score in top_10]\n",
    "    top_10_texts = [data[\"docs\"][doc_id].text for doc_id in top_10_ids]\n",
    "    reranked = ranker.rank(query=query, docs=top_10_texts, doc_ids=top_10_ids)\n",
    "    doc_ids = [result.doc_id for result in reranked]\n",
    "    scores = [result.score for result in reranked]\n",
    "    doc_ranking = list(zip(doc_ids, scores))\n",
    "\n",
    "    return doc_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_results = pd.DataFrame(columns=(\"Dataset\", \"Size\", \"MRR\", \"Runtime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mrr2 = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for query_id in test_query_ids:\n",
    "    doc_ranking = model(query_id, True, True)\n",
    "    average_mrr2 += calculateMRR(query_id, doc_ranking, relevant_docs)\n",
    "    \n",
    "average_mrr2 /= len(test_query_ids)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "reranker_results.loc[0] = [\"Test\", len(test_query_ids), average_mrr2, execution_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado recente:\n",
    "- Com processamento dos documentos\n",
    "- Com processamento das queries\n",
    "\n",
    "Obs.: O processamento inclui remover qualquer caractere que não seja uma letra, número, underscore ou espaço em branco; e deixar em lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Size</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>555</td>\n",
       "      <td>0.593363</td>\n",
       "      <td>312.731814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset  Size       MRR     Runtime\n",
       "0    Test   555  0.593363  312.731814"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranker_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado anterior:\n",
    "- Com processamento dos documentos\n",
    "- Sem processamento das queries (apenas lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Size</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>555</td>\n",
       "      <td>0.582853</td>\n",
       "      <td>315.65561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset  Size       MRR    Runtime\n",
       "0    Test   555  0.582853  315.65561"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranker_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_results.to_csv(\"../results/reranker_results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
