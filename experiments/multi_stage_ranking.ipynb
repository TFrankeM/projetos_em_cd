{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import ir_datasets\n",
    "import time\n",
    "from rerankers import Reranker\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Split em conjuntos**\n",
    "\n",
    "Datasets de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "def load_dataset(input_file):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "data_set = \"../subset_msmarco_train_0/subset_msmarco_train_0.01_99.pkl\"\n",
    "\n",
    "data = load_dataset(data_set)\n",
    "queries = data[\"queries\"]\n",
    "\n",
    "# Split the queries (assuming queries is a dictionary of {query_id: query_object})\n",
    "query_ids = list(queries.keys())  # List of query IDs\n",
    "\n",
    "# Shuffle query IDs to ensure a random split\n",
    "random.shuffle(query_ids)\n",
    "\n",
    "# Split into 80% for training, 20% for validation\n",
    "split_ratio = 0.8\n",
    "train_query_ids = query_ids[:int(len(query_ids) * split_ratio)]\n",
    "test_query_ids = query_ids[int(len(query_ids) * split_ratio):]\n",
    "\n",
    "train_queries = {qid: queries[qid] for qid in train_query_ids}\n",
    "test_queries = {qid: queries[qid] for qid in test_query_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tratamento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário agrupa os documentos relevantes por consulta (query)\n",
    "relevant_docs = dict()\n",
    "\n",
    "for qrel in data[\"qrels\"]:\n",
    "    relevant_docs[qrel.query_id] = relevant_docs.get(qrel.query_id, []) + [qrel.doc_id]\n",
    "\n",
    "# e.g.: relevant_docs = {'query_id': ['doc_id_1', 'doc_id_12']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifica os documentos relevantes no conjunto de treinamentom \n",
    "# removendo duplicatas\n",
    "train_docs = set()\n",
    "for qid in train_query_ids:\n",
    "    train_docs.update(relevant_docs[qid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **mean reciprocal rank** is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: $1$ for first place, $\\frac{1}{2}$ for second place, $\\frac{1}{3}$ for third place and so on. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:\n",
    "\n",
    "$$\n",
    "MRR =\n",
    "\\frac{1}{\\|Q\\|} = \\sum_{i=1}^{\\|Q\\|} \\frac{1}{rank_i}\n",
    "$$\n",
    "\n",
    "where ${\\displaystyle {\\text{rank}}_{i}}$ refers to the rank position of the first relevant document for the i-th query.\n",
    "\n",
    "The reciprocal value of the mean reciprocal rank corresponds to the harmonic mean of the ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMRR(query_id, ranking):\n",
    "    \"\"\"\n",
    "    Calcula o MRR (Mean Reciprocal Rank) para uma consulta.\n",
    "    \n",
    "    :param query_id: ID da query atual.\n",
    "    :param ranking: Lista de documentos recuperados ordenados.\n",
    "    \n",
    "    :return: MRR para a consulta.\n",
    "    \"\"\"\n",
    "\n",
    "    for i, doc in enumerate(ranking):\n",
    "        if doc[0] in relevant_docs[query_id]:\n",
    "            return 1 / (i + 1)\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantDocTexts(query_id):\n",
    "    \"\"\"\n",
    "    Retorna os textos dos documentos relevantes para uma consulta.\n",
    "    \n",
    "    :param query_id: ID da query.\n",
    "    \n",
    "    :return: Lista de textos dos documentos relevantes.\n",
    "    \"\"\"\n",
    "    \n",
    "    relevant_doc_texts = []\n",
    "    for doc_id in relevant_docs[query_id]:\n",
    "        relevant_doc_texts.append(data[\"docs\"][doc_id].text)\n",
    "\n",
    "    return relevant_doc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, remove_punctuation = True, lowercase = True):\n",
    "    \"\"\"\n",
    "    Função para preprocessamento de texto: remove pontuação, converte para minúsculas e divide em tokens.\n",
    "    \n",
    "    :param text: Texto a ser processado.\n",
    "    :remove_punctuation bool: Para True remove pontuação\n",
    "    :lowercase bool: Para True coloca em lowercase\n",
    "\n",
    "    :return: Lista de tokens do texto.\n",
    "    \"\"\"\n",
    "\n",
    "    # [^\\w\\s] corresponde a qualquer caractere que não seja uma letra, número, underscore ou espaço em branco\n",
    "    # mantendo apenas letras, números e espaços\n",
    "\n",
    "    # Caso 1\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    # Caso 2\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelo baseline**\n",
    "\n",
    "## **BM25**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O BM25Okapi é uma implementação do BM25, um modelo clássico utilizado em sistemas de recuperação de informações (como motores de busca) para avaliar a relevância de documentos em relação a uma consulta (query).\n",
    "\n",
    "O BM25 é uma fórmula de pontuação de relevância que calcula o quão bem um documento corresponde a uma consulta com base nas palavras que o documento contém.\n",
    "\n",
    "Ele é um modelo probabilístico de recuperação de informações, baseado na ideia de que a relevância de um documento para uma consulta depende da frequência das palavras que aparecem no documento e na consulta, mas com uma diminuição das contribuições das palavras que ocorrem com frequência excessiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [preprocess(doc.text) for doc in data[\"docs\"].values()]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(query_id, remove_punctuation, lowercase):\n",
    "    query = queries[query_id].text\n",
    "    tokenized_query = preprocess(query, remove_punctuation, lowercase)\n",
    "\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    doc_ranking = sorted(zip(data[\"docs\"].keys(), doc_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return doc_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_25_results = pd.DataFrame(columns=(\"Dataset\", \"Size\", \"MRR\", \"Runtime (sec)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mrr = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for query_id in test_query_ids:\n",
    "    doc_ranking = baseline_model(query_id, True, True)\n",
    "    average_mrr += calculateMRR(query_id, doc_ranking)\n",
    "\n",
    "average_mrr /= len(test_query_ids)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "bm_25_results.loc[0] = [\"Test\", len(test_query_ids), average_mrr, execution_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mrr = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for query_id in train_query_ids:\n",
    "    doc_ranking = baseline_model(query_id, True, True)\n",
    "    average_mrr += calculateMRR(query_id, doc_ranking)\n",
    "\n",
    "average_mrr /= len(train_query_ids)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "bm_25_results.loc[1] = [\"Train\", len(train_query_ids), average_mrr, execution_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mrr = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for query_id in query_ids:\n",
    "    doc_ranking = baseline_model(query_id, True, True)\n",
    "    average_mrr += calculateMRR(query_id, doc_ranking)\n",
    "\n",
    "average_mrr /= len(query_ids)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "bm_25_results.loc[2] = [\"Full\", len(query_ids), average_mrr, execution_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado recente:\n",
    "- Com processamento dos documentos\n",
    "- Com processamento das queries\n",
    "\n",
    "Obs.: O processamento inclui remover qualquer caractere que não seja uma letra, número, underscore ou espaço em branco; e deixar em lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Estudo de escalabilidade:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Size</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Runtime (sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>555</td>\n",
       "      <td>0.488720</td>\n",
       "      <td>298.350545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train</td>\n",
       "      <td>2216</td>\n",
       "      <td>0.449240</td>\n",
       "      <td>1220.195126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Full</td>\n",
       "      <td>2771</td>\n",
       "      <td>0.457147</td>\n",
       "      <td>1467.348227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset  Size       MRR  Runtime (sec)\n",
       "0    Test   555  0.488720     298.350545\n",
       "1   Train  2216  0.449240    1220.195126\n",
       "2    Full  2771  0.457147    1467.348227"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"==> Estudo de escalabilidade:\")\n",
    "bm_25_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_25_results.to_csv(\"../results/bm_25_results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado anterior:\n",
    "- Com processamento dos documentos\n",
    "- Sem processamento das queries (apenas lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Size</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>555</td>\n",
       "      <td>0.474292</td>\n",
       "      <td>257.141707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train</td>\n",
       "      <td>2216</td>\n",
       "      <td>0.428867</td>\n",
       "      <td>1072.796115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Full</td>\n",
       "      <td>2771</td>\n",
       "      <td>0.437965</td>\n",
       "      <td>1340.438707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset  Size       MRR      Runtime\n",
       "0    Test   555  0.474292   257.141707\n",
       "1   Train  2216  0.428867  1072.796115\n",
       "2    Full  2771  0.437965  1340.438707"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"==> Estudo de escalabilidade:\")\n",
    "bm_25_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ablação na etapa de tratamento dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_25_processing_results = pd.DataFrame(columns=(\"Remove punctuation\", \"Lowercase\", \"MRR\", \"Runtime (sec)\"))\n",
    "index = 0\n",
    "\n",
    "for remove_punctuation in [True, False]:\n",
    "    for lowercase in [True, False]:\n",
    "        average_mrr = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for query_id in test_query_ids:\n",
    "            doc_ranking = baseline_model(query_id, remove_punctuation, lowercase)\n",
    "            average_mrr += calculateMRR(query_id, doc_ranking)\n",
    "\n",
    "        average_mrr /= len(test_query_ids)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "\n",
    "        bm_25_processing_results.loc[index] = [remove_punctuation, lowercase, average_mrr, execution_time]\n",
    "\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Estudo do desempenho do tratamento de dados:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Remove punctuation</th>\n",
       "      <th>Lowercase</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Runtime (sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.488720</td>\n",
       "      <td>297.235475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.488860</td>\n",
       "      <td>289.510962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.474292</td>\n",
       "      <td>297.298308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.474432</td>\n",
       "      <td>288.388891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Remove punctuation  Lowercase       MRR  Runtime (sec)\n",
       "0                True       True  0.488720     297.235475\n",
       "1                True      False  0.488860     289.510962\n",
       "2               False       True  0.474292     297.298308\n",
       "3               False      False  0.474432     288.388891"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"==> Estudo do desempenho do tratamento de dados:\")\n",
    "bm_25_processing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_25_processing_results.to_csv(\"../results/bm_25_processing_results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelo Reranker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default cross-encoder model for language en\n",
      "Warning: Model type could not be auto-mapped with the defaults list. Defaulting to TransformerRanker.\n",
      "If your model is NOT intended to be ran as a one-label cross-encoder, please reload it and specify the model_type! Otherwise, you may ignore this warning. You may specify `model_type='cross-encoder'` to suppress this warning in the future.\n",
      "Default Model: mixedbread-ai/mxbai-rerank-base-v1\n",
      "Loading TransformerRanker model mixedbread-ai/mxbai-rerank-base-v1 (this message can be suppressed by setting verbose=0)\n",
      "No dtype set\n",
      "Using dtype torch.float32\n",
      "Loaded model mixedbread-ai/mxbai-rerank-base-v1\n",
      "Using device cuda.\n",
      "Using dtype torch.float32.\n"
     ]
    }
   ],
   "source": [
    "ranker = Reranker(\"cross-encoder\", device='cuda')\n",
    "def model(query_id, remove_punctuation, lowercase):\n",
    "    query = queries[query_id].text\n",
    "    tokenized_query = preprocess(query, remove_punctuation, lowercase)\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    doc_ranking = sorted(zip(data[\"docs\"].keys(), doc_scores), key=lambda x: x[1], reverse=True)\n",
    "    top_10 = doc_ranking[:10]\n",
    "    top_10_ids = [doc_id for doc_id, score in top_10]\n",
    "    top_10_texts = [data[\"docs\"][doc_id].text for doc_id in top_10_ids]\n",
    "    reranked = ranker.rank(query=query, docs=top_10_texts, doc_ids=top_10_ids)\n",
    "    doc_ids = [result.doc_id for result in reranked]\n",
    "    scores = [result.score for result in reranked]\n",
    "    doc_ranking = list(zip(doc_ids, scores))\n",
    "\n",
    "    return doc_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_results = pd.DataFrame(columns=(\"Dataset\", \"Size\", \"MRR\", \"Runtime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mrr2 = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for query_id in test_query_ids:\n",
    "    doc_ranking = model(query_id, True, True)\n",
    "    average_mrr2 += calculateMRR(query_id, doc_ranking)\n",
    "    \n",
    "average_mrr2 /= len(test_query_ids)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "reranker_results.loc[0] = [\"Test\", len(test_query_ids), average_mrr2, execution_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado recente:\n",
    "- Com processamento dos documentos\n",
    "- Com processamento das queries\n",
    "\n",
    "Obs.: O processamento inclui remover qualquer caractere que não seja uma letra, número, underscore ou espaço em branco; e deixar em lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Size</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>555</td>\n",
       "      <td>0.593363</td>\n",
       "      <td>312.731814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset  Size       MRR     Runtime\n",
       "0    Test   555  0.593363  312.731814"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranker_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado anterior:\n",
    "- Com processamento dos documentos\n",
    "- Sem processamento das queries (apenas lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Size</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test</td>\n",
       "      <td>555</td>\n",
       "      <td>0.582853</td>\n",
       "      <td>315.65561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset  Size       MRR    Runtime\n",
       "0    Test   555  0.582853  315.65561"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranker_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_results.to_csv(\"../results/reranker_results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
